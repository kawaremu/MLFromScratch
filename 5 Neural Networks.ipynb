{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Neural Networks Learning\n",
    "\n",
    "In this notebook, you will implement the backpropagation algorithm for neural networks and apply it to the task of hand-written digit recognition. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# donn√©es\n",
    "data = np.genfromtxt('data/neural_nets/data.csv', delimiter=',', dtype=float)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ces donn√©es (data), les 400 premi√®res colonnes representent les pixels de l'image (20x20), la derni√®re colonne represente la classe de l'image (chiffres de 0 √† 9). (http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "Chaque ligne represente un exemple de notre ensemble de donn√©es. \n",
    "\n",
    "Mettons ces donn√©es dans leus vecteurs correspondants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rajoutons l'ordonn√©e √† l'origine theta 0\n",
    "intercept=np.ones((data.shape[0],1))\n",
    "X=np.column_stack((intercept,data[:,:-1]))\n",
    "y=data[:,-1]\n",
    "# forcer y √† avoir une seule colonne\n",
    "y = y.reshape( y.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X', X.shape ,' y ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation al√©atoire de quelques donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "for i in range(13):\n",
    "    c = random.randint(X.shape[0])\n",
    "    a = X[c,1:].reshape((20, 20))\n",
    "    a=np.transpose(a)\n",
    "    plt.subplot(1,13,i+1)\n",
    "    plt.title('label '+ str(y[c]))\n",
    "    plt.imshow(a,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 1, r√©gression logistique\n",
    "Dans cette partie, nous aimerions utiliser la r√©gression logistique pour classifier nos images\n",
    "\n",
    "Rappelons que la r√©gression logistique nous donne la probabilit√© d'appartenance (oui ou non) √† la classe 1 (elle permet une classification binaire).\n",
    "\n",
    "Pour √©tendre la r√©gression logistique √† une multi-classification, nous allons utiliser une strat√©gie 1 contre tous.\n",
    "\n",
    "Nous param√®tres $\\theta$ seront donc une matrice avec un nombre de lignes √©gale au nombre de classes, et avec un nombre de colones √©gale au nombre decaract√©ristiques (chaque ligne de la matrice $\\theta$ correspond aux param√®tres d'un classifieur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrCostFunction (X, y, initial_theta, alpha, MaxIter,lambda_):\n",
    "    # dans cette fonction vous devez appliquer la regression logistique avec tout ce que ceci implique\n",
    "    # calcul du co√ªt, minimisation du co√ªt avec descente du gradient, et retour des param√®tres theta pour une classe\n",
    "    theta = np.array(initial_theta, copy=True)\n",
    "    for i in range(MaxIter):\n",
    "        y_hat = Sigmoid(X @ theta)\n",
    "        error = y_hat - y\n",
    "        theta_grad = np.expand_dims((X * error).mean(axis=0), axis=1)\n",
    "        theta[0] = theta[0] - alpha * theta_grad[0]\n",
    "        theta[1:] = theta[1:] * (1 - lambda_ * alpha / X.shape[0]) - alpha * theta_grad[1:]\n",
    "    return theta.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictOnveVsAll (all_theta, X):\n",
    "    # ici en utilisant les param√®tres calcul√©s par la r√©gression logisitique, \n",
    "    # nous aiemrions retourner les etiquettes predites\n",
    "    \n",
    "    # Ici chaque classifieur retournera de probabilit√©, il faudra choisir \n",
    "    # la probabilit√© maximale \n",
    "    # de tous les classifieurs d'une exemple donn√©\n",
    "    # r√©peter pour tous les exmemples\n",
    "    y_pred = np.argmax(Sigmoid(X @ all_theta.T), axis=1)\n",
    "    \n",
    "    return classes[y_pred[..., np.newaxis]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes= np.unique(y)\n",
    "number_classes=classes.shape[0]\n",
    "all_theta = np.zeros((number_classes, X.shape[1]));\n",
    "all_theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxIter= 10000\n",
    "lambda_= 0.1\n",
    "alpha = 0.01\n",
    "# initial_theta pour chaque classifieur\n",
    "initial_theta=np.zeros((X.shape[1], 1));\n",
    "for i in range (number_classes):\n",
    "     # appel pour chaque classifieur\n",
    "     theta = lrCostFunction(X,(y==classes[i]).astype(int),initial_theta,alpha, MaxIter,lambda_);\n",
    "     all_theta[i,:]=theta;               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predictOnveVsAll(all_theta, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualit√© du classifieur RL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = np.mean(y==y_pred)*100\n",
    "precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 2: R√©seaux de neurones\n",
    "Pour cette partie, nous choisissons une r√©seau simple:\n",
    "\n",
    "- une couche d'entr√©e avec 400 noeuds (20 x 20 pixels) + le biais\n",
    "- une couche cach√©e avec 25 noeuds\n",
    "- une couche de sortie avec 10 noeuds (nombre de classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poids de la couche 1\n",
    "W1 = np.genfromtxt('W1.csv', delimiter=',', dtype=float)\n",
    "W1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poids de la couche 2\n",
    "W2 = np.genfromtxt('W2.csv', delimiter=',', dtype=float)\n",
    "W2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size  = 400;  \n",
    "hidden_layer_size = 25;   \n",
    "num_labels = 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pr√©diction**\n",
    "\n",
    "Appliquer une propagation en avant en utilisant les param√®tres donn√©es pour pr√©dir les classes de l'ensemble d'apprentissage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict (W1, W2, X):\n",
    "    # appliquer une propagation en avant\n",
    "    # !--- n'oubliez pas d'appliquer la sigmoid √† chaque couche afin d'avoir les probabilit√©s ---!\n",
    "    \n",
    "    # pr√©dire la classe en choisissant la probabilit√© maximale parmi les 10 noeuds de sortie\n",
    "    \n",
    "    h1_output = Sigmoid(X @ W1.T)\n",
    "    \n",
    "    intercept = np.ones((X.shape[0],1))\n",
    "    h2_input = np.column_stack((intercept, h1_output))\n",
    "    \n",
    "    h2_output = Sigmoid(h2_input @ W2.T)\n",
    "    \n",
    "    y_pred = np.argmax(h2_output, axis=1)[..., np.newaxis]\n",
    "    \n",
    "    y_pred = y_pred + 1\n",
    "    y_pred[y_pred==10] = 0\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=predict(X)\n",
    "precision = np.mean(y==y_pred)*100\n",
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V√©rification de l'implementation\n",
    "Comparer vos algorithmes √† ceux de scikitlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty=\"l2\", max_iter=10000)\n",
    "model.fit(X[:, 1:], y.squeeze())\n",
    "y_prob = model.predict_proba(X[:, 1:])\n",
    "y_pred = np.argmax(y_prob, axis=1)[..., np.newaxis]\n",
    "\n",
    "np.mean(y_pred==y)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 6 R√©seaux de neurones - Multi classification et Propagation en arri√®re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rajoutons l'ordonn√©e √† l'origine theta 0\n",
    "intercept=np.ones((data.shape[0],1))\n",
    "X=np.column_stack((intercept,data[:,:-1]))\n",
    "y=data[:,-1]\n",
    "# forcer y √† avoir une seule colonne\n",
    "y = y.reshape( y.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X', X.shape ,' y ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer y de tel sorte √† avoir un vecteur pour chaque exemple\n",
    "\n",
    "Equivalent de tocategorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YY=np.zeros((y.shape[0], int(np.max(y))+1))\n",
    "YY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (YY.shape[0]):\n",
    "    YY[i,int(y[i])]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "for i in range(13):\n",
    "    c = random.randint(X.shape[0])\n",
    "    a = X[c,1:].reshape((20, 20))\n",
    "    a=np.transpose(a)\n",
    "    plt.subplot(1,13,i+1)\n",
    "    plt.title('label '+ str(y[c]))\n",
    "    plt.imshow(a,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R√©seaux de neurones\n",
    "Pour cette partie, nous choisissons une r√©seau simple:\n",
    "\n",
    "- une couche d'entr√©e avec 400 noeuds (20 x 20 pixels) + le biais\n",
    "- une couche cach√©e avec 25 noeuds\n",
    "- une couche de sortie avec 10 noeuds (nombre de classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# poids de la couche 1\n",
    "W1 = np.genfromtxt('data/neural_nets/W1.csv', delimiter=',', dtype=float)\n",
    "W1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poids de la couche 2\n",
    "W2 = np.genfromtxt('data/neural_nets/W2.csv', delimiter=',', dtype=float)\n",
    "W2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size  = 400;  \n",
    "hidden_layer_size = 25;   \n",
    "num_labels = 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO  1 calcul du co√ªt\n",
    "**T√¢che 1**: Modifier la fonction computeCost afin d'obtenir un co√ªt avec r√©gularisation\n",
    "\n",
    "Rappelons que le co√ªt avec r√©gularisation est calcul√© comme suit:\n",
    "\n",
    "$ J(\\theta)= \\frac{1}{m} \\sum_{i=1}^m\\sum_{k=1}^K \\left[ -y_k^{(i)} \\log (h_\\theta(x^{(i)}) )_k - (1-y_k^{(i)}) \\log (1-h_\\theta(x^{(i)}) )_k \\right] + \\frac{\\lambda}{2m} \\left[ \\sum_{j=1}^{25}\\sum_{k=1}^{400} \\left( \\theta_{j,k}^{(1)} \\right)^2 +   \\sum_{j=1}^{10}\\sum_{k=1}^{25} \\left( \\theta_{j,k}^{(2)} \\right)^2 \\right] $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid(z):\n",
    "    # pour une valeur donn√©e, cette fonction calculera sa sigmoid\n",
    "    return 1 / (1 + np.exp(-z));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(X, YY, theta1, theta2, hidden_layer_size, lambda_):\n",
    "    a1 = X\n",
    "\n",
    "    z2 = np.zeros((X.shape[0], hidden_layer_size))\n",
    "    a2 = np.zeros((X.shape[0], hidden_layer_size))\n",
    "\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    z2 = np.column_stack((intercept, np.matmul(a1, np.transpose(theta1))))\n",
    "    a2 = Sigmoid(z2)\n",
    "    a2[:, 0] = 1\n",
    "\n",
    "    z3 = np.zeros((YY.shape[0], YY.shape[1]))\n",
    "    a3 = np.zeros((YY.shape[0], YY.shape[1]))\n",
    "\n",
    "    z3 = np.matmul(a2, np.transpose(theta2))\n",
    "    a3 = Sigmoid(z3)\n",
    "\n",
    "    J = np.zeros((YY.shape[0],1))\n",
    "    J = (1 / y.shape[0]) * (np.sum(np.sum((-YY * np.log(a3)) - ((1 - YY) * np.log(1 - a3))))) + \\\n",
    "        (lambda_ / (2 * X.shape[0])) * (np.power(theta1[:, 1:], 2).sum() + np.power(theta2[:, 1:], 2).sum())\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO  2 calcul du gradient\n",
    "**T√¢che 2**: Impl√©menter la fonction NNCostFunction afin de retourner:\n",
    "\n",
    "- Le co√ªt avec r√©gularisation\n",
    "- Le gardient du co√ªt par rapport √† chaqu'un des param√®tres du r√©seau\n",
    "\n",
    "Rappelons le gradient de la sigmoid:\n",
    "\n",
    "$ sigmoid(z) =\\sigma(z)= \\frac{1}{1-e^{-z}}$\n",
    "\n",
    "$ \\sigma(z)^\\prime = \\sigma(z)(1-\\sigma(z)) =a(1-a)$ tel que $a$ est l'activation d'une couche donn√©e\n",
    "\n",
    "L'algorithme √† implementer est comme suit:\n",
    "\n",
    "Pour chaque exemple de l'ensemble d'apprentissage faire\n",
    "\n",
    "Pour chaque noeud de la couche de sortie, calculer la d√©riv√©e (gradient):\n",
    "\n",
    "$\\delta_k^{(3)} = \\left( a_k^{(3)}- y_k \\right)  $\n",
    "\n",
    "Pour chaque noeud de la couche cach√© calculer la d√©riv√©e (gradient):\n",
    "\n",
    "$\\delta^{(2)} =  \\left(\\Theta^{(2)}\\right)^T \\delta^{(3)} \\quad .* \\quad \\sigma^\\prime\\left(z^{(2)} \\right) $\n",
    "\n",
    "Notons que l'op√©rateur $.*$ represente la multiplication √©lement par √©lement et non pas la multiplication matricielle\n",
    "\n",
    "Le gradient de chaque noeud et de chaque couche sera finalement:\n",
    "\n",
    "$\\Delta^{(l)} =  \\Delta^{(l)} + \\delta^{(l)} * \\left( a^{(l)} \\right)^T $\n",
    "\n",
    "Fin pour\n",
    "\n",
    "Fin pour\n",
    "\n",
    "Diviser le gradient cumul√© par le nombre d'exemples:\n",
    "\n",
    "$ \\frac{\\partial J(\\Theta)} { \\partial \\Theta_{i,j)}^{(l)}} = \\frac{1}{m} \\Delta_{i,j)}^{(l)}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NNCostFunction(X, YY, theta1, theta2, hidden_layer_size, lambda_ = 0):\n",
    "    a1 = X # 5000x401\n",
    "    z2 = X @ theta1.T # 5000x401 @ 401x25 = 5000x25\n",
    "    a2 = np.column_stack((np.ones((z2.shape[0], 1)), Sigmoid(z2))) # 5000x26\n",
    "\n",
    "    z3 = a2 @ theta2.T # 5000x26 @ 26x10 = 5000x10\n",
    "    a3 = Sigmoid(z3) # 5000x10\n",
    "\n",
    "    delta3 = a3 - YY # 5000x10\n",
    "\n",
    "    delta2 = ((delta3 @ theta2) * a2 * (1 - a2))[:, 1:] \n",
    "\n",
    "    theta2_grad = (np.broadcast_to(delta3[:, :, np.newaxis], (*delta3.shape, a2.shape[1])) \\\n",
    "        * np.broadcast_to(a2[:, np.newaxis, :], (a2.shape[0], delta3.shape[1], a2.shape[1]))).mean(axis=0) \\\n",
    "        + (lambda_ / X.shape[0]) * np.column_stack((np.zeros((theta2.shape[0], 1)), theta2[:, 1:])) \n",
    "\n",
    "    theta1_grad = (np.broadcast_to(delta2[:, :, np.newaxis], (*delta2.shape, a1.shape[1])) \\\n",
    "        * np.broadcast_to(a1[:, np.newaxis, :], (a1.shape[0], delta2.shape[1], a1.shape[1]))).mean(axis=0) \\\n",
    "        + (lambda_ / X.shape[0]) * np.column_stack((np.zeros((theta1.shape[0], 1)), theta1[:, 1:])) \n",
    "\n",
    "    J = computeCost(X, YY, theta1, theta2, hidden_layer_size, lambda_)\n",
    "    \n",
    "    return J, [theta1_grad, theta2_grad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO  3 entrainement du r√©seau\n",
    "**T√¢che 3**: Entrainer le r√©seau en utilisant la descente du gradient:\n",
    "Ici il faudra reprendre l'algorithme de la descente du gradient afin de mettre √† jour les param√®tres du r√©seau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha, iterations, lambda_ = 0):\n",
    "    losses, gradients = [], []\n",
    "    for _ in range(iterations):\n",
    "        loss, grad = NNCostFunction(X, y, theta[0], theta[1], hidden_layer_size, lambda_)\n",
    "        \n",
    "        theta[0] = theta[0] - alpha * grad[0]\n",
    "        theta[1] = theta[1] - alpha * grad[1]\n",
    "        \n",
    "        losses.append(loss)\n",
    "        gradients.append((grad[0].mean(), grad[1].mean()))\n",
    "        \n",
    "    return theta, np.asarray(losses), np.asarray(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost_gradient(costs, gradients):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"costs\")\n",
    "    plt.plot(range(costs.shape[0]), costs)\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('cost')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"gradients\")\n",
    "    for i in range(gradients.shape[1]):\n",
    "        plt.plot(range(gradients.shape[0]), gradients[:, i])\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('gardient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 500\n",
    "alpha = 1.5\n",
    "theta = [np.random.normal(0, np.sqrt(1/212), W1.shape), np.random.normal(0, np.sqrt(1/17), W2.shape)]\n",
    "\n",
    "theta, losses, gradients = gradientDescent(X, YY, theta, alpha, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cost_gradient(losses, gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparaison avec les poids W1, W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "computeCost(X, YY, theta[0], theta[1], hidden_layer_size, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO 4 : R√©gularisation\n",
    "Mettre √† jour le calcul du co√ªt en ajoutant le terme de r√©gularisation et comparer les r√©sultats (avec et sans r√©gularisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iterations = 500\n",
    "alpha = 1.5\n",
    "lambda_ = 500 \n",
    "theta_reg = [np.random.normal(0, np.sqrt(1/212), W1.shape), np.random.normal(0, np.sqrt(1/17), W2.shape)]\n",
    "\n",
    "theta_reg, losses_reg, gradients_reg = gradientDescent(X, YY, theta_reg, alpha, iterations, lambda_=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_cost_gradient(losses_reg, gradients_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "computeCost(X, YY, theta_reg[0], theta_reg[1], hidden_layer_size, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l'erreur est un peu plus √©lev√©e que celle sans r√©gularisation. La pr√©cision est plus basse aussi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO 5 : Pr√©diction\n",
    "\n",
    "Appliquer une propagation en avant en utilisant les param√®tres donn√©es pour pr√©dir les classes de l'ensemble d'apprentissage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W1, W2, X):\n",
    "    \n",
    "    # appliquer une propagation en avant\n",
    "    # !--- n'oubliez pas d'appliquer la sigmoid √† chaque couche afin d'avoir les probabilit√©s ---!\n",
    "    \n",
    "    # pr√©dire la classe en choisissant la probabilit√© maximale parmi les 10 noeuds de sortie\n",
    "    h1_output = Sigmoid(X @ W1.T)\n",
    "    \n",
    "    intercept = np.ones((X.shape[0],1))\n",
    "    h2_input = np.column_stack((intercept, h1_output))\n",
    "    \n",
    "    h2_output = Sigmoid(h2_input @ W2.T)\n",
    "    \n",
    "    y_pred = np.argmax(h2_output, axis=1)[..., np.newaxis]\n",
    "    \n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La pr√©cision du mod√®le :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(theta[0], theta[1], X)\n",
    "precision = np.mean(y==y_pred)*100\n",
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La pr√©cision du mod√®le avec r√©gularisation (qui est plus bas que celui du mod√®le sans):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(theta_reg[0], theta_reg[1], X)\n",
    "precision = np.mean(y==y_pred)*100\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(theta_reg[0], theta_reg[1], X)\n",
    "precision = np.mean(y==y_pred)*100\n",
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optionnel 1 : V√©rification de l'implementation\n",
    "Comparer vos algorithmes √† ceux de scikitlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='sgd', alpha=0.01, activation='logistic', max_iter=5000,\n",
    "        hidden_layer_sizes=(25,), random_state=42)\n",
    "clf.fit(X[:, 1:], YY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(clf.predict(X[:, 1:]), axis=1)[..., np.newaxis]\n",
    "precision = np.mean(y==y_pred)*100\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.score(X[:, 1:], YY) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optionnel 2 : Visualisation des poids\n",
    "    \n",
    "La visualisation de poids permet de voir quel partie du r√©seau est activ√© et pour quelle classe.\n",
    "Il est possible de visulaiser les param√®tres theta1 . \n",
    "Ceci peut se faire en utilisant un reshape de ces param√®tres afin d'avoir 25 images de taille 20x20 (n'oubliez pas d'ignorer la premu√®re colonne, celle du biais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_weights(w1, w2):\n",
    "    plt.figure(figsize=(50, 20))\n",
    "    for i in range(w1.shape[0]):\n",
    "        plt.subplot(3, 13, i + 1)\n",
    "        plt.imshow(np.reshape(w1[i][1:], (20, 20)))\n",
    "    for i in range(w2.shape[0]):\n",
    "        plt.subplot(3, 13, w1.shape[0] + i + 2)\n",
    "        plt.imshow(np.reshape(w2[i][1:], (-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_weights(theta[0], theta[1])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d48602314cfe5a9900ec6b938bb325d180d9d20e0954f86c002443dd920ba8a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
